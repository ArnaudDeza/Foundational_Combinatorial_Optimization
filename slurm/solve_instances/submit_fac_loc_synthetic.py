#!/usr/bin/env python3
"""
SLURM submission script for solving facility location problem instances.
Supports both UFLP and P-median problems generated by synthetic_generator.py.
"""

import argparse
import json
import logging
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import glob

# Add the project root to Python path
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.insert(0, str(project_root))


def discover_instance_files(
    data_directory: str,
    problem_type: Optional[str] = None,
    recursive: bool = True
) -> List[str]:
    """Discover facility location instance files in a directory."""
    pattern = "**/*.pkl" if recursive else "*.pkl"
    pkl_files = glob.glob(os.path.join(data_directory, pattern), recursive=recursive)
    
    # Filter out result files
    instance_files = [f for f in pkl_files if "_results.pkl" not in f]
    
    # Filter by problem type if specified
    if problem_type is not None:
        filtered_files = []
        for file in instance_files:
            basename = os.path.basename(file)
            # More flexible matching - check if problem type is in filename
            if (problem_type.lower() in basename.lower() or 
                (problem_type == 'pmedian' and 'k_' in basename) or  # P-median has k parameter
                (problem_type == 'uflp' and 'k_' not in basename)):  # UFLP doesn't have k parameter
                filtered_files.append(file)
        instance_files = filtered_files
    
    return sorted(instance_files)


def check_results_exist(instance_file: str, problem_type: str, results_folder: Optional[str] = None) -> bool:
    """Check if results already exist for this instance file and problem type."""
    base_name = os.path.basename(instance_file)
    prefix = base_name.replace(".pkl", "")
    
    if results_folder is None:
        results_folder = os.path.dirname(instance_file)
    
    result_file = os.path.join(results_folder, f"{prefix}_{problem_type}_results.pkl")
    return os.path.exists(result_file)


def validate_arguments(args):
    """Validate command line arguments."""
    # Only validate data_directory if it's provided
    if hasattr(args, 'data_directory') and args.data_directory is not None:
        if not os.path.exists(args.data_directory):
            raise ValueError(f"Data directory does not exist: {args.data_directory}")
    
    # Validate instance files if provided
    if hasattr(args, 'instance_files') and args.instance_files is not None:
        for file in args.instance_files:
            if not os.path.exists(file):
                raise ValueError(f"Instance file does not exist: {file}")
    
    if args.results_folder and not os.path.exists(args.results_folder):
        logging.warning(f"Results folder does not exist, will be created: {args.results_folder}")
    
    if args.time_limit <= 0:
        raise ValueError("Time limit must be positive")
    
    if args.max_threads <= 0:
        raise ValueError("Max threads must be positive")
    
    if args.mem_gb <= 0:
        raise ValueError("Memory must be positive")


def create_slurm_script(
    job_name: str,
    instance_files: List[str],
    problem_type: str,
    time_limit: int,
    max_threads: int,
    mem_gb: int,
    mip_gap: Optional[float],
    mip_focus: Optional[int],
    results_folder: Optional[str],
    overwrite: bool,
    log_level: str,
    partition: str,
    slurm_time: str
) -> str:
    """Create SLURM batch script content."""
    
    # Use absolute path to solve_instances.py
    solve_script_path = str(project_root / "src" / "fac_loc" / "solve_instances.py")
    
    # Build solve_instances.py command
    solve_cmd = [
        "python", solve_script_path,
        "--instance_files"
    ]
    
    # Add instance files (properly quoted)
    for file in instance_files:
        solve_cmd.append(f'"{file}"')
    
    solve_cmd.extend([
        "--problem_type", problem_type,
        "--time_limit", str(time_limit),
        "--max_threads", str(max_threads),
        "--log_level", log_level
    ])
    
    if mip_gap is not None:
        solve_cmd.extend(["--mip_gap", str(mip_gap)])
    
    if mip_focus is not None:
        solve_cmd.extend(["--mip_focus", str(mip_focus)])
    
    if results_folder:
        solve_cmd.extend(["--results_folder", f'"{results_folder}"'])
    
    if overwrite:
        solve_cmd.append("--overwrite")
    
    solve_cmd_str = " ".join(solve_cmd)
    
    # Format results folder check
    results_check = ""
    if results_folder:
        results_check = f"""
# Create results folder if needed
if [ ! -d "{results_folder}" ]; then
    mkdir -p "{results_folder}"
fi"""
    
    script_content = f"""#!/bin/bash
#SBATCH --job-name={job_name}
#SBATCH --partition={partition}
#SBATCH --time={slurm_time}
#SBATCH --ntasks=1
#SBATCH --cpus-per-task={max_threads}
#SBATCH --mem={mem_gb}G
#SBATCH --output=logs/{job_name}_%j.out
#SBATCH --error=logs/{job_name}_%j.err

# Load required modules (adjust based on your cluster)
# module load python/3.9
# module load gurobi/10.0.0

# Set up environment
export PYTHONPATH={project_root}:$PYTHONPATH
# export GUROBI_HOME=/opt/gurobi1000/linux64
# export PATH=$GUROBI_HOME/bin:$PATH
# export LD_LIBRARY_PATH=$GUROBI_HOME/lib:$LD_LIBRARY_PATH
{results_check}

# Print job information
echo "Job started at: $(date)"
echo "Running on node: $(hostname)"
echo "Job ID: $SLURM_JOB_ID"
echo "Problem type: {problem_type}"
echo "Number of instance files: {len(instance_files)}"
echo "Time limit per instance: {time_limit} seconds"
echo "Max threads: {max_threads}"
echo "Memory allocated: {mem_gb}G"

# Change to project directory
cd {project_root}

# Run the solver
echo "Starting facility location solving..."
{solve_cmd_str}

# Check completion status
if [ $? -eq 0 ]; then
    echo "Job completed successfully at: $(date)"
else
    echo "Job failed at: $(date)"
    exit 1
fi
"""
    
    return script_content


def submit_job(
    script_content: str,
    job_name: str,
    dry_run: bool = False
) -> Optional[str]:
    """Submit SLURM job and return job ID."""
    
    # Create logs directory if it doesn't exist
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # Write script to temporary file
    script_file = f"temp_{job_name}.slurm"
    
    try:
        with open(script_file, 'w') as f:
            f.write(script_content)
        
        if dry_run:
            print(f"Dry run - would submit job with script:")
            print("-" * 50)
            print(script_content)
            print("-" * 50)
            return None
        
        # Submit job
        result = subprocess.run(
            ["sbatch", script_file],
            capture_output=True,
            text=True,
            check=True
        )
        
        # Extract job ID from output
        output = result.stdout.strip()
        job_id = output.split()[-1] if "Submitted batch job" in output else None
        
        return job_id
        
    except subprocess.CalledProcessError as e:
        logging.error(f"Failed to submit job: {e}")
        logging.error(f"Error output: {e.stderr}")
        return None
    
    finally:
        # Clean up temporary script file
        if os.path.exists(script_file):
            os.remove(script_file)


def main():
    """Main entry point for SLURM submission."""
    parser = argparse.ArgumentParser(
        description="Submit facility location solving jobs to SLURM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Submit UFLP jobs for all instances in a directory
  python submit_fac_loc_synthetic.py --data_directory /path/to/uflp/instances --problem_type uflp
  
  # Submit P-median jobs with custom settings
  python submit_fac_loc_synthetic.py --data_directory /path/to/pmedian/instances --problem_type pmedian --time_limit 1800 --mem_gb 8
  
  # Submit specific files with high priority
  python submit_fac_loc_synthetic.py --instance_files instance1.pkl instance2.pkl --problem_type uflp --partition high_priority
        """
    )
    
    # Input specification (mutually exclusive)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--data_directory", type=str,
                            help="Directory containing instance files")
    input_group.add_argument("--instance_files", type=str, nargs="+",
                            help="Specific instance files to solve")
    
    # Problem configuration
    parser.add_argument("--problem_type", type=str, required=True,
                       choices=["uflp", "uncapacitated_facility_location", "pmedian", "p-median", "p_median"],
                       help="Type of facility location problem to solve")
    
    # Solver parameters
    parser.add_argument("--time_limit", type=int, default=3600,
                       help="Time limit in seconds for each instance (default: 3600)")
    parser.add_argument("--max_threads", type=int, default=2,
                       help="Maximum threads for each Gurobi solve (default: 2)")
    parser.add_argument("--mip_gap", type=float, default=None,
                       help="MIP optimality gap tolerance")
    parser.add_argument("--mip_focus", type=int, default=None,
                       help="Gurobi MIP focus setting (0-3)")
    
    # SLURM parameters
    parser.add_argument("--partition", type=str, default="normal",
                       help="SLURM partition to use (default: normal)")
    parser.add_argument("--slurm_time", type=str, default="6:00:00",
                       help="SLURM time limit (default: 6:00:00)")
    parser.add_argument("--mem_gb", type=int, default=4,
                       help="Memory in GB (default: 4)")
    parser.add_argument("--job_prefix", type=str, default="fac_loc",
                       help="Prefix for job names (default: fac_loc)")
    
    # Job management
    parser.add_argument("--instances_per_job", type=int, default=10,
                       help="Number of instances per SLURM job (default: 10)")
    parser.add_argument("--overwrite", action="store_true",
                       help="Overwrite existing result files")
    parser.add_argument("--dry_run", action="store_true",
                       help="Show what would be submitted without actually submitting")
    
    # Output options
    parser.add_argument("--results_folder", type=str, default=None,
                       help="Folder to save results (default: same as instance files)")
    
    # Logging
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="Logging level (default: INFO)")
    
    args = parser.parse_args()
    
    # Configure logging
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    # Validate arguments
    try:
        validate_arguments(args)
    except ValueError as e:
        logging.error(f"Invalid arguments: {e}")
        return 1
    
    # Discover instance files
    if args.data_directory:
        logging.info(f"Discovering instance files in {args.data_directory}")
        instance_files = discover_instance_files(
            args.data_directory,
            problem_type=args.problem_type,
            recursive=True
        )
        logging.info(f"Found {len(instance_files)} instance files")
    else:
        instance_files = args.instance_files
        logging.info(f"Processing {len(instance_files)} specified instance files")
    
    if not instance_files:
        logging.error("No instance files found or specified")
        return 1
    
    # Filter out files that already have results if not overwriting
    if not args.overwrite:
        pending_files = []
        for file in instance_files:
            if not check_results_exist(file, args.problem_type, args.results_folder):
                pending_files.append(file)
            else:
                logging.info(f"Skipping {file} - results already exist")
        instance_files = pending_files
    
    if not instance_files:
        logging.info("No pending files to process")
        return 0
    
    logging.info(f"Will process {len(instance_files)} instance files")
    
    # Split instance files into chunks for parallel jobs
    chunks = []
    for i in range(0, len(instance_files), args.instances_per_job):
        chunk = instance_files[i:i + args.instances_per_job]
        chunks.append(chunk)
    
    logging.info(f"Created {len(chunks)} job chunks with up to {args.instances_per_job} instances each")
    
    # Submit jobs
    submitted_jobs = []
    
    for i, chunk in enumerate(chunks):
        job_name = f"{args.job_prefix}_{args.problem_type}_{i+1:03d}"
        
        logging.info(f"Preparing job {i+1}/{len(chunks)}: {job_name} ({len(chunk)} instances)")
        
        # Create SLURM script
        script_content = create_slurm_script(
            job_name=job_name,
            instance_files=chunk,
            problem_type=args.problem_type,
            time_limit=args.time_limit,
            max_threads=args.max_threads,
            mem_gb=args.mem_gb,
            mip_gap=args.mip_gap,
            mip_focus=args.mip_focus,
            results_folder=args.results_folder,
            overwrite=args.overwrite,
            log_level=args.log_level,
            partition=args.partition,
            slurm_time=args.slurm_time
        )
        
        # Submit job
        job_id = submit_job(script_content, job_name, args.dry_run)
        
        if job_id:
            submitted_jobs.append((job_name, job_id))
            logging.info(f"Submitted job {job_name} with ID {job_id}")
        elif not args.dry_run:
            logging.error(f"Failed to submit job {job_name}")
    
    # Summary
    if args.dry_run:
        print(f"\nDry run complete. Would submit {len(chunks)} jobs for {len(instance_files)} instances.")
    else:
        print(f"\nSubmitted {len(submitted_jobs)} jobs for {len(instance_files)} instances:")
        for job_name, job_id in submitted_jobs:
            print(f"  {job_name}: {job_id}")
        
        if submitted_jobs:
            print(f"\nMonitor jobs with: squeue -u $USER")
            print(f"Check job logs in: logs/")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
