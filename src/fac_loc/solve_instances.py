#!/usr/bin/env python3
"""
Facility Location Instance Solving Script.
Compatible with instances generated by synthetic_generator.py.
"""

import argparse
import json
import logging
import os
import pickle
import glob
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

# Local imports
from solvers import (
    get_solver, 
    process_facility_location_file,
    validate_solution,
    SOLVER_REGISTRY
)

# Optional imports for parallel processing
try:
    from joblib import Parallel, delayed
    HAS_JOBLIB = True
except ImportError:
    HAS_JOBLIB = False
    print("Warning: joblib not available. Parallel processing disabled.")

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("Warning: tqdm not available. Progress bars disabled.")
    def tqdm(iterable, *args, **kwargs):
        return iterable

try:
    from tqdm_joblib import tqdm_joblib
    HAS_TQDM_JOBLIB = True
except ImportError:
    HAS_TQDM_JOBLIB = False
    def tqdm_joblib(tqdm_object):
        class DummyContext:
            def __enter__(self):
                return self
            def __exit__(self, *args):
                pass
        return DummyContext()


def discover_instance_files(
    data_directory: str, 
    recursive: bool = True,
    problem_type: Optional[str] = None
) -> List[str]:
    """
    Discover facility location instance files in a directory.
    
    Args:
        data_directory: Directory to search for instance files
        recursive: Whether to search recursively 
        problem_type: Filter for specific problem type (uflp or pmedian)
        
    Returns:
        List of instance file paths
    """
    pattern = "**/*.pkl" if recursive else "*.pkl"
    pkl_files = glob.glob(os.path.join(data_directory, pattern), recursive=recursive)
    
    # Filter out result files
    instance_files = [f for f in pkl_files if "_results.pkl" not in f]
    
    # Filter by problem type if specified
    if problem_type is not None:
        filtered_files = []
        for file in instance_files:
            basename = os.path.basename(file)
            if problem_type.lower() in basename.lower():
                filtered_files.append(file)
        instance_files = filtered_files
    
    return sorted(instance_files)


def check_existing_results(instance_file: str, problem_type: str, results_folder: Optional[str] = None) -> bool:
    """Check if results already exist for this instance file and problem type."""
    base_name = os.path.basename(instance_file)
    prefix = base_name.replace(".pkl", "")
    
    if results_folder is None:
        results_folder = os.path.dirname(instance_file)
    
    result_file = os.path.join(results_folder, f"{prefix}_{problem_type}_results.pkl")
    return os.path.exists(result_file)


def process_instances_parallel(
    instance_files: List[str],
    problem_type: str,
    solver_kwargs: Dict,
    nparallel: int = 1,
    results_folder: Optional[str] = None,
    overwrite: bool = False
) -> List:
    """Process multiple instance files in parallel."""
    
    # Filter out files that already have results if not overwriting
    if not overwrite:
        pending_files = []
        for file in instance_files:
            if not check_existing_results(file, problem_type, results_folder):
                pending_files.append(file)
            else:
                logging.info(f"Skipping {file} - results already exist")
        instance_files = pending_files
    
    if not instance_files:
        logging.info("No files to process")
        return []
    
    logging.info(f"Processing {len(instance_files)} instance files with {nparallel} parallel workers")
    
    if nparallel == 1 or not HAS_JOBLIB:
        # Sequential processing
        results = []
        for file in tqdm(instance_files, desc=f"Solving {problem_type}", unit="file"):
            result = process_facility_location_file(file, problem_type, solver_kwargs, results_folder)
            results.append(result)
    else:
        # Parallel processing
        if HAS_TQDM_JOBLIB:
            with tqdm_joblib(tqdm(total=len(instance_files), 
                                 desc=f"Solving {problem_type}", 
                                 unit="file")):
                results = Parallel(n_jobs=nparallel)(
                    delayed(process_facility_location_file)(
                        file, problem_type, solver_kwargs, results_folder
                    )
                    for file in instance_files
                )
        else:
            results = Parallel(n_jobs=nparallel)(
                delayed(process_facility_location_file)(
                    file, problem_type, solver_kwargs, results_folder
                )
                for file in instance_files
            )
    
    return results


def generate_summary_report(results: List, output_file: Optional[str] = None) -> Dict:
    """Generate a summary report of solving results."""
    summary = {
        'total_files': len(results),
        'successful_files': 0,
        'failed_files': 0,
        'total_instances': 0,
        'successful_instances': 0,
        'failed_instances': 0,
        'valid_solutions': 0,
        'invalid_solutions': 0,
        'average_runtime': 0.0,
        'average_objective': 0.0,
        'solver_status_counts': {},
        'errors': []
    }
    
    all_runtimes = []
    all_objectives = []
    
    for result in results:
        if isinstance(result, dict) and 'error' in result:
            summary['failed_files'] += 1
            summary['errors'].append(result['error'])
            continue
        
        summary['successful_files'] += 1
        
        if isinstance(result, list):
            for instance_result in result:
                summary['total_instances'] += 1
                
                # Check if solving was successful
                status = instance_result.get('mip_status', -1)
                if status in [2, 9]:  # Optimal or time limit with solution
                    summary['successful_instances'] += 1
                    
                    # Runtime statistics
                    runtime = instance_result.get('mip_runtime', 0)
                    all_runtimes.append(runtime)
                    
                    # Objective statistics
                    obj_val = instance_result.get('mip_obj_val', float('inf'))
                    if obj_val != float('inf'):
                        all_objectives.append(obj_val)
                else:
                    summary['failed_instances'] += 1
                
                # Solution validation
                if instance_result.get('solution_valid', False):
                    summary['valid_solutions'] += 1
                else:
                    summary['invalid_solutions'] += 1
                
                # Status counts
                status_str = str(status)
                summary['solver_status_counts'][status_str] = summary['solver_status_counts'].get(status_str, 0) + 1
    
    # Calculate averages
    if all_runtimes:
        summary['average_runtime'] = np.mean(all_runtimes)
    if all_objectives:
        summary['average_objective'] = np.mean(all_objectives)
    
    # Save summary if requested
    if output_file:
        with open(output_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        logging.info(f"Summary report saved to {output_file}")
    
    return summary


def print_summary(summary: Dict):
    """Print a formatted summary of results."""
    print("\n" + "="*60)
    print("FACILITY LOCATION SOLVING SUMMARY")
    print("="*60)
    print(f"Files processed: {summary['successful_files']}/{summary['total_files']}")
    print(f"Instances solved: {summary['successful_instances']}/{summary['total_instances']}")
    print(f"Valid solutions: {summary['valid_solutions']}/{summary['total_instances']}")
    
    if summary['average_runtime'] > 0:
        print(f"Average runtime: {summary['average_runtime']:.2f} seconds")
    if summary['average_objective'] > 0:
        print(f"Average objective: {summary['average_objective']:.2f}")
    
    if summary['solver_status_counts']:
        print("\nSolver status distribution:")
        for status, count in summary['solver_status_counts'].items():
            print(f"  Status {status}: {count} instances")
    
    if summary['errors']:
        print(f"\nErrors encountered: {len(summary['errors'])}")
        for error in summary['errors'][:5]:  # Show first 5 errors
            print(f"  - {error}")
        if len(summary['errors']) > 5:
            print(f"  ... and {len(summary['errors']) - 5} more")
    
    print("="*60)


def main():
    """Main entry point for facility location instance solving."""
    parser = argparse.ArgumentParser(
        description="Solve facility location problem instances",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Solve all UFLP instances in a directory
  python solve_instances.py --data_directory /path/to/uflp/instances --problem_type uflp
  
  # Solve P-median instances with custom settings
  python solve_instances.py --data_directory /path/to/pmedian/instances --problem_type pmedian --time_limit 1800 --nparallel 4
  
  # Solve specific instance files
  python solve_instances.py --instance_files instance1.pkl instance2.pkl --problem_type uflp
        """
    )
    
    # Input specification (mutually exclusive)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument("--data_directory", type=str,
                            help="Directory containing instance files")
    input_group.add_argument("--instance_files", type=str, nargs="+",
                            help="Specific instance files to solve")
    
    # Problem configuration
    parser.add_argument("--problem_type", type=str, required=True,
                       choices=list(SOLVER_REGISTRY.keys()),
                       help="Type of facility location problem to solve")
    
    # Solver parameters
    parser.add_argument("--time_limit", type=int, default=3600,
                       help="Time limit in seconds for each instance (default: 3600)")
    parser.add_argument("--max_threads", type=int, default=1,
                       help="Maximum threads for each Gurobi solve (default: 1)")
    parser.add_argument("--mip_gap", type=float, default=None,
                       help="MIP optimality gap tolerance")
    parser.add_argument("--mip_focus", type=int, default=None,
                       help="Gurobi MIP focus setting (0-3)")
    
    # Processing parameters
    parser.add_argument("--nparallel", type=int, default=1,
                       help="Number of parallel file processors (default: 1)")
    parser.add_argument("--recursive", action="store_true",
                       help="Search directories recursively")
    parser.add_argument("--overwrite", action="store_true",
                       help="Overwrite existing result files")
    
    # Output options
    parser.add_argument("--results_folder", type=str, default=None,
                       help="Folder to save results (default: same as instance files)")
    parser.add_argument("--summary_file", type=str, default=None,
                       help="Save summary report to JSON file")
    
    # Logging
    parser.add_argument("--log_level", type=str, default="INFO",
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="Logging level (default: INFO)")
    
    args = parser.parse_args()
    
    # Configure logging
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    # Discover instance files
    if args.data_directory:
        logging.info(f"Discovering instance files in {args.data_directory}")
        instance_files = discover_instance_files(
            args.data_directory, 
            recursive=args.recursive,
            problem_type=args.problem_type
        )
        logging.info(f"Found {len(instance_files)} instance files")
    else:
        instance_files = args.instance_files
        logging.info(f"Processing {len(instance_files)} specified instance files")
    
    if not instance_files:
        logging.error("No instance files found or specified")
        return
    
    # Prepare solver configuration
    solver_kwargs = {
        'time_limit': args.time_limit,
        'max_threads': args.max_threads,
        'mip_gap': args.mip_gap,
        'mip_focus': args.mip_focus
    }
    
    # Process instances
    logging.info(f"Starting {args.problem_type} solving with configuration: {solver_kwargs}")
    
    results = process_instances_parallel(
        instance_files=instance_files,
        problem_type=args.problem_type,
        solver_kwargs=solver_kwargs,
        nparallel=args.nparallel,
        results_folder=args.results_folder,
        overwrite=args.overwrite
    )
    
    # Generate and display summary
    summary = generate_summary_report(results, args.summary_file)
    print_summary(summary)
    
    logging.info("Facility location solving completed")


if __name__ == "__main__":
    main() 